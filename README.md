# README #


### What is this repository for? ###

* Web crawler which crawls a given URL and extracts all pages which are marked with 'terms and conditions', 'privacy', 'legal'
* Creates a folder of the given website and stores each page data into a separate .txt file


### How do I get set up? ###

* Load up the files on a web server
* Ensure that the server allows for mkdir function
* PHP 6.2 + required (still works with older but haven't really tested)


### Notes ###

* Right now this only works when a URL in manually entered into text field. 
* Will work on automatically finding sites and automate the process